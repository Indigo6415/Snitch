#!/usr/bin/env python3

from dependencies.snitch import SnitchEngine, SnitchResult
from dependencies.fetch import FetchURL, SnitchContent
import dependencies.prefix as prefix
import argparse
import time

parser = argparse.ArgumentParser(description="SnitchEngine URL Fetcher")
parser.add_argument("--url", type=str, default="https://example.com/", help="URL to fetch")
parser.add_argument("--verbose", action="store_true", help="Enable verbose output")
parser.add_argument("--use_headers", action="store_true", help="Use headers in fetch requests")
parser.add_argument("--slow_mode", action="store_true", help="Enable slow mode (1 second delay between requests)")
parser.add_argument("--recursion_depth", type=int, default=0, help="Set recursion depth")

args = parser.parse_args()

url = args.url
verbose = args.verbose
use_headers = args.use_headers
slow_mode = args.slow_mode
recursion_depth = args.recursion_depth

# url = "http://localhost:9999/index.html"

def main():
    extracted_secrets: list[SnitchResult] = []
    snitch_content: SnitchContent = SnitchContent()
    snitch_content.add_new_link(url)

    for recursion in range(0, recursion_depth + 1):
        # Make room for the new new links, not the old links.
        temp_new_links = snitch_content.new_links.copy()
        snitch_content.merge_new_and_extracted_links()
        snitch_content.clear_new_links()

        # Only fetch the content of the URL if it has not been fetched before.
        for link in temp_new_links:
            if slow_mode: time.sleep(1)
            if verbose: print("="*50)

            # ==================================================
            # Retrieve the content of a URL
            # ==================================================
            content = FetchURL(link, use_headers=use_headers, verbose=verbose).fetch()
            if not content:
                continue
            # Save extracted content to an array
            snitch_content.add_extracted_content((link, content))

            # ==================================================
            # Start snitching using the SnitchEngine
            # ==================================================
            snitch_engine = SnitchEngine(content, verbose=verbose)

            # Extract links from the content
            links = snitch_engine.extract_links()
            # Save new links to array
            snitch_content.add_new_link(links)
            if not links and verbose:
                print(f"{prefix.warning} No links found in {prefix.cyan}{content.url}{prefix.reset}")

            # Extract JS from the content
            js = snitch_engine.extract_js()
            # Save new JS links to array
            snitch_content.add_new_link(js)
            if not js and verbose:
                print(f"{prefix.warning} No JavaScript files found in {prefix.cyan}{content.url}{prefix.reset}")

            print(f"{prefix.info} Extracted {prefix.yellow}{len(links)}{prefix.reset} links and {prefix.yellow}{len(js)}{prefix.reset} JavaScript files")

    if verbose: print("="*50)

    # Extract secrets from the content
    for content in snitch_content.extracted_content:
        snitch_engine = SnitchEngine(content[1], verbose=False)


        # Extract secrets from the content
        secrets = snitch_engine.extract_secrets(regex=True, entropy=False, entropy_threshold=4.5, char_limit=200, ai=False, ai_threshold=0.9)
        regex_secrets = secrets["regex"]
        entropy_secrets = secrets["entropy"]
        ai_secrets = secrets["ai"]

        for secret in regex_secrets: # Dictionary in a list
            extracted_secrets.append(SnitchResult(content[0], "Regex", secret[0], secret[1]))

        for secret in entropy_secrets: # Normal list
            extracted_secrets.append(SnitchResult(content[0], "Entropy", "Unknown", secret))

        for secret in ai_secrets: # Normal list
            extracted_secrets.append(SnitchResult(content[0], "AI", "Unknown", secret))

    # Remove any duplicates from the extracted secrets
    # unique_secrets: list = []
    # for content in extracted_secrets:
    #     if content in unique_secrets:
    #         continue
    #     unique_secrets.append(content)

    for secret in extracted_secrets:
        print(f"Location: {prefix.yellow}{secret.location}{prefix.reset}")
        print(f"Method: {prefix.cyan}{secret.method}{prefix.reset}")
        print(f"Type: {prefix.red}{secret.type}{prefix.reset}")
        print(f"Secret: {prefix.green}{secret.secret}{prefix.reset}")
        print()



if __name__ == "__main__":
    # TODO: Create flag to save fetched content to disk.
    main()
